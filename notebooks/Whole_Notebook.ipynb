{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12336338,"sourceType":"datasetVersion","datasetId":7776597},{"sourceId":12343304,"sourceType":"datasetVersion","datasetId":7781400},{"sourceId":12343759,"sourceType":"datasetVersion","datasetId":7781712},{"sourceId":12346950,"sourceType":"datasetVersion","datasetId":7783707},{"sourceId":455241,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":369210,"modelId":390083}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import DistilBertTokenizerFast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:19.940606Z","iopub.execute_input":"2025-07-01T09:13:19.941055Z","iopub.status.idle":"2025-07-01T09:13:19.945135Z","shell.execute_reply.started":"2025-07-01T09:13:19.941028Z","shell.execute_reply":"2025-07-01T09:13:19.944304Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#****Testing if it works despite the pip warnings ****\nimport torch\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\nprint(\"DistilBERT loaded ✅\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:19.946081Z","iopub.execute_input":"2025-07-01T09:13:19.946358Z","iopub.status.idle":"2025-07-01T09:13:42.119415Z","shell.execute_reply.started":"2025-07-01T09:13:19.946332Z","shell.execute_reply":"2025-07-01T09:13:42.118647Z"}},"outputs":[{"name":"stderr","text":"2025-07-01 09:13:26.551912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751361206.746603      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751361206.805195      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Torch: 2.6.0+cu124\nCUDA available: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e83923f6a8c46619eeea947cdef0e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"626709b043fc438d879af5bee595e246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644330e5387e40f5a17249e7f3f0fe91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f4899be4e4e494f80389604b4944b7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2e6b1ae17854e5d907f2eb2f48ce48f"}},"metadata":{}},{"name":"stdout","text":"DistilBERT loaded ✅\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Reimport if needed\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Re-create crisis report DataFrame\ndata = {\n    \"text\": [\n        \"Elderly man collapsed in Noida clinic.\",\n        \"Need urgent food in flood relief camp in Guwahati.\",\n        \"Rescue needed for family stuck in waterlogged building.\",\n        \"Storm destroyed homes. Need shelter in Cuttack.\",\n        \"Medical emergency: woman fainted during heatwave.\",\n        \"Children stranded without food in Chennai slums.\",\n        \"People stuck on rooftop in Assam.\",\n        \"No medicine available in clinic near Lajpat Nagar.\",\n        \"Request for shelter after cyclone hit Orissa.\",\n        \"Need drinking water and food packets in Bihar village.\",\n        \"Collapsed house in landslide — people injured.\",\n        \"Camp needs volunteers for elderly care.\",\n        \"Overcrowded shelter in Bhopal — need mattresses.\",\n        \"Power and water outage in rain-affected area.\",\n        \"Medical staff needed in rural health center.\",\n        \"Flood victims need cooked meals in Assam.\",\n        \"Doctors required in mobile ambulance unit.\",\n        \"Families cold, without blankets in shelter zone.\",\n        \"Urgent food supply needed in Kolkata outskirts.\",\n        \"Ambulance stuck in traffic, patient critical.\"\n    ],\n    \"type\": [\n        \"Medical\", \"Food\", \"Rescue\", \"Shelter\", \"Medical\",\n        \"Food\", \"Rescue\", \"Medical\", \"Shelter\", \"Food\",\n        \"Rescue\", \"Medical\", \"Shelter\", \"Other\", \"Medical\",\n        \"Food\", \"Medical\", \"Shelter\", \"Food\", \"Medical\"\n    ],\n    \"urgency\": [\n        \"High\", \"High\", \"High\", \"Medium\", \"High\",\n        \"High\", \"High\", \"Medium\", \"Medium\", \"Medium\",\n        \"High\", \"Medium\", \"Low\", \"Low\", \"Medium\",\n        \"Medium\", \"High\", \"Medium\", \"Medium\", \"High\"\n    ]\n}\n\ndf = pd.DataFrame(data)\n\n# Apply label encoding\ntype_encoder = LabelEncoder()\nurgency_encoder = LabelEncoder()\n\ndf[\"type_encoded\"] = type_encoder.fit_transform(df[\"type\"])\ndf[\"urgency_encoded\"] = urgency_encoder.fit_transform(df[\"urgency\"])\n\n# Confirm it's fixed\nprint(\"✅ Encoded columns created!\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:42.141526Z","iopub.execute_input":"2025-07-01T09:13:42.141781Z","iopub.status.idle":"2025-07-01T09:13:42.162527Z","shell.execute_reply.started":"2025-07-01T09:13:42.141756Z","shell.execute_reply":"2025-07-01T09:13:42.161614Z"}},"outputs":[{"name":"stdout","text":"✅ Encoded columns created!\n                                                text     type urgency  \\\n0             Elderly man collapsed in Noida clinic.  Medical    High   \n1  Need urgent food in flood relief camp in Guwah...     Food    High   \n2  Rescue needed for family stuck in waterlogged ...   Rescue    High   \n3    Storm destroyed homes. Need shelter in Cuttack.  Shelter  Medium   \n4  Medical emergency: woman fainted during heatwave.  Medical    High   \n\n   type_encoded  urgency_encoded  \n0             1                0  \n1             0                0  \n2             3                0  \n3             4                2  \n4             1                0  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass CrisisDataset(Dataset):\n    def __init__(self, texts, type_labels, urgency_labels, tokenizer):\n        self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n        self.labels_type = torch.tensor(type_labels)\n        self.labels_urgency = torch.tensor(urgency_labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item[\"labels_type\"] = self.labels_type[idx]\n        item[\"labels_urgency\"] = self.labels_urgency[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels_type)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:42.163196Z","iopub.execute_input":"2025-07-01T09:13:42.163375Z","iopub.status.idle":"2025-07-01T09:13:42.181562Z","shell.execute_reply.started":"2025-07-01T09:13:42.163361Z","shell.execute_reply":"2025-07-01T09:13:42.180853Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Recreate dataset\ndataset = CrisisDataset(\n    texts=df[\"text\"].tolist(),\n    type_labels=df[\"type_encoded\"].tolist(),\n    urgency_labels=df[\"urgency_encoded\"].tolist(),\n    tokenizer=tokenizer\n)\n\n# Recreate dataloader\nfrom torch.utils.data import DataLoader\nloader = DataLoader(dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:42.182369Z","iopub.execute_input":"2025-07-01T09:13:42.182645Z","iopub.status.idle":"2025-07-01T09:13:42.203935Z","shell.execute_reply.started":"2025-07-01T09:13:42.182623Z","shell.execute_reply":"2025-07-01T09:13:42.203100Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn as nn\nfrom transformers import DistilBertModel\n\n# Rebuild the classifier model if needed\nclass CrisisClassifier(nn.Module):\n    def __init__(self, num_types, num_urgencies):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.dropout = nn.Dropout(0.3)\n        self.type_head = nn.Linear(self.bert.config.hidden_size, num_types)\n        self.urgency_head = nn.Linear(self.bert.config.hidden_size, num_urgencies)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = self.dropout(output.last_hidden_state[:, 0])\n        return self.type_head(pooled), self.urgency_head(pooled)\n\n# Device config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize model\nmodel = CrisisClassifier(\n    num_types=len(type_encoder.classes_),\n    num_urgencies=len(urgency_encoder.classes_)\n).to(device)\n\n# Optimizer and loss\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(150):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels_type = batch[\"labels_type\"].to(device)\n        labels_urgency = batch[\"labels_urgency\"].to(device)\n\n        out_type, out_urgency = model(input_ids, attention_mask)\n        loss_type = loss_fn(out_type, labels_type)\n        loss_urgency = loss_fn(out_urgency, labels_urgency)\n        loss = loss_type + loss_urgency\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1} — Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:13:42.204740Z","iopub.execute_input":"2025-07-01T09:13:42.205018Z","iopub.status.idle":"2025-07-01T09:14:00.175814Z","shell.execute_reply.started":"2025-07-01T09:13:42.204994Z","shell.execute_reply":"2025-07-01T09:14:00.175184Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 — Loss: 13.2671\nEpoch 2 — Loss: 12.2461\nEpoch 3 — Loss: 10.9518\nEpoch 4 — Loss: 9.9822\nEpoch 5 — Loss: 8.8285\nEpoch 6 — Loss: 7.2396\nEpoch 7 — Loss: 5.9548\nEpoch 8 — Loss: 4.3966\nEpoch 9 — Loss: 3.7855\nEpoch 10 — Loss: 2.9365\nEpoch 11 — Loss: 2.3064\nEpoch 12 — Loss: 1.8674\nEpoch 13 — Loss: 1.5039\nEpoch 14 — Loss: 1.2751\nEpoch 15 — Loss: 1.0021\nEpoch 16 — Loss: 0.9229\nEpoch 17 — Loss: 0.7505\nEpoch 18 — Loss: 0.7112\nEpoch 19 — Loss: 0.5918\nEpoch 20 — Loss: 0.5447\nEpoch 21 — Loss: 0.4532\nEpoch 22 — Loss: 0.5115\nEpoch 23 — Loss: 0.3894\nEpoch 24 — Loss: 0.3648\nEpoch 25 — Loss: 0.3231\nEpoch 26 — Loss: 0.3548\nEpoch 27 — Loss: 0.3577\nEpoch 28 — Loss: 0.3120\nEpoch 29 — Loss: 0.2672\nEpoch 30 — Loss: 0.2457\nEpoch 31 — Loss: 0.2616\nEpoch 32 — Loss: 0.2569\nEpoch 33 — Loss: 0.2145\nEpoch 34 — Loss: 0.2034\nEpoch 35 — Loss: 0.2147\nEpoch 36 — Loss: 0.2035\nEpoch 37 — Loss: 0.1992\nEpoch 38 — Loss: 0.1732\nEpoch 39 — Loss: 0.1824\nEpoch 40 — Loss: 0.1827\nEpoch 41 — Loss: 0.1807\nEpoch 42 — Loss: 0.1562\nEpoch 43 — Loss: 0.1604\nEpoch 44 — Loss: 0.1683\nEpoch 45 — Loss: 0.1564\nEpoch 46 — Loss: 0.1436\nEpoch 47 — Loss: 0.1367\nEpoch 48 — Loss: 0.1409\nEpoch 49 — Loss: 0.1275\nEpoch 50 — Loss: 0.1396\nEpoch 51 — Loss: 0.1292\nEpoch 52 — Loss: 0.1254\nEpoch 53 — Loss: 0.1216\nEpoch 54 — Loss: 0.1052\nEpoch 55 — Loss: 0.1058\nEpoch 56 — Loss: 0.1156\nEpoch 57 — Loss: 0.0957\nEpoch 58 — Loss: 0.0982\nEpoch 59 — Loss: 0.1083\nEpoch 60 — Loss: 0.0975\nEpoch 61 — Loss: 0.1020\nEpoch 62 — Loss: 0.1037\nEpoch 63 — Loss: 0.0934\nEpoch 64 — Loss: 0.0977\nEpoch 65 — Loss: 0.0872\nEpoch 66 — Loss: 0.0803\nEpoch 67 — Loss: 0.0788\nEpoch 68 — Loss: 0.0948\nEpoch 69 — Loss: 0.0829\nEpoch 70 — Loss: 0.0858\nEpoch 71 — Loss: 0.0781\nEpoch 72 — Loss: 0.0802\nEpoch 73 — Loss: 0.0768\nEpoch 74 — Loss: 0.0756\nEpoch 75 — Loss: 0.0742\nEpoch 76 — Loss: 0.0743\nEpoch 77 — Loss: 0.0768\nEpoch 78 — Loss: 0.0686\nEpoch 79 — Loss: 0.0674\nEpoch 80 — Loss: 0.0789\nEpoch 81 — Loss: 0.0699\nEpoch 82 — Loss: 0.0646\nEpoch 83 — Loss: 0.0771\nEpoch 84 — Loss: 0.0673\nEpoch 85 — Loss: 0.0547\nEpoch 86 — Loss: 0.0671\nEpoch 87 — Loss: 0.0717\nEpoch 88 — Loss: 0.0619\nEpoch 89 — Loss: 0.0610\nEpoch 90 — Loss: 0.0583\nEpoch 91 — Loss: 0.0568\nEpoch 92 — Loss: 0.0589\nEpoch 93 — Loss: 0.0571\nEpoch 94 — Loss: 0.0548\nEpoch 95 — Loss: 0.0486\nEpoch 96 — Loss: 0.0504\nEpoch 97 — Loss: 0.0486\nEpoch 98 — Loss: 0.0558\nEpoch 99 — Loss: 0.0616\nEpoch 100 — Loss: 0.0572\nEpoch 101 — Loss: 0.0510\nEpoch 102 — Loss: 0.0496\nEpoch 103 — Loss: 0.0532\nEpoch 104 — Loss: 0.0463\nEpoch 105 — Loss: 0.0461\nEpoch 106 — Loss: 0.0505\nEpoch 107 — Loss: 0.0528\nEpoch 108 — Loss: 0.0579\nEpoch 109 — Loss: 0.0445\nEpoch 110 — Loss: 0.0469\nEpoch 111 — Loss: 0.0465\nEpoch 112 — Loss: 0.0491\nEpoch 113 — Loss: 0.0432\nEpoch 114 — Loss: 0.0460\nEpoch 115 — Loss: 0.0424\nEpoch 116 — Loss: 0.0416\nEpoch 117 — Loss: 0.0436\nEpoch 118 — Loss: 0.0443\nEpoch 119 — Loss: 0.0417\nEpoch 120 — Loss: 0.0353\nEpoch 121 — Loss: 0.0438\nEpoch 122 — Loss: 0.0391\nEpoch 123 — Loss: 0.0427\nEpoch 124 — Loss: 0.0352\nEpoch 125 — Loss: 0.0383\nEpoch 126 — Loss: 0.0404\nEpoch 127 — Loss: 0.0439\nEpoch 128 — Loss: 0.0402\nEpoch 129 — Loss: 0.0413\nEpoch 130 — Loss: 0.0405\nEpoch 131 — Loss: 0.0334\nEpoch 132 — Loss: 0.0379\nEpoch 133 — Loss: 0.0399\nEpoch 134 — Loss: 0.0408\nEpoch 135 — Loss: 0.0336\nEpoch 136 — Loss: 0.0377\nEpoch 137 — Loss: 0.0366\nEpoch 138 — Loss: 0.0413\nEpoch 139 — Loss: 0.0408\nEpoch 140 — Loss: 0.0310\nEpoch 141 — Loss: 0.0374\nEpoch 142 — Loss: 0.0362\nEpoch 143 — Loss: 0.0360\nEpoch 144 — Loss: 0.0310\nEpoch 145 — Loss: 0.0356\nEpoch 146 — Loss: 0.0381\nEpoch 147 — Loss: 0.0295\nEpoch 148 — Loss: 0.0336\nEpoch 149 — Loss: 0.0327\nEpoch 150 — Loss: 0.0336\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 🔍 Sample unseen crisis reports\ndef classify_crisis(text):\n    model.eval()\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n\n    with torch.no_grad():\n        out_type, out_urgency = model(**inputs)\n        type_pred = torch.argmax(out_type, dim=1).item()\n        urgency_pred = torch.argmax(out_urgency, dim=1).item()\n\n    return {\n        \"text\": text,\n        \"type\": type_encoder.inverse_transform([type_pred])[0],\n        \"urgency\": urgency_encoder.inverse_transform([urgency_pred])[0]\n    }\n\n\nnew_reports = [\n    \"Flood waters rising in Siliguri, 3 families trapped on rooftop.\",\n    \"Urgent requirement of medical aid for collapsed patient in Kanpur.\",\n    \"Children hungry, no food for 2 days in relief shelter, Bihar.\",\n    \"Need shelter urgently after heavy rains in Meghalaya.\",\n    \"Water pipes broken, entire slum has no clean drinking water.\",\n    \"Food packets needed in Malda. People haven’t eaten since 2 days.\",\n    \"Flood rescue team needed in Aligarh. Locals stranded.\",\n    \"Doctor unavailable in community clinic. Multiple patients fainted.\",\n    \"Request for blankets and warm clothes in hilly areas of Shimla.\",\n    \"Family of 4 needs urgent evacuation from waterlogged home in Cuttack.\"\n]\n\n# Batch classify and collect results\nresults = []\n\nfor report in new_reports:\n    res = classify_crisis(report)\n    results.append(res)\n\n# Convert to DataFrame\ndf_preds = pd.DataFrame(results)\ndf_preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:00.176500Z","iopub.execute_input":"2025-07-01T09:14:00.176762Z","iopub.status.idle":"2025-07-01T09:14:00.271091Z","shell.execute_reply.started":"2025-07-01T09:14:00.176743Z","shell.execute_reply":"2025-07-01T09:14:00.270476Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                text     type urgency\n0  Flood waters rising in Siliguri, 3 families tr...   Rescue    High\n1  Urgent requirement of medical aid for collapse...  Medical    High\n2  Children hungry, no food for 2 days in relief ...     Food  Medium\n3  Need shelter urgently after heavy rains in Meg...  Shelter  Medium\n4  Water pipes broken, entire slum has no clean d...  Medical  Medium\n5  Food packets needed in Malda. People haven’t e...     Food  Medium\n6  Flood rescue team needed in Aligarh. Locals st...     Food    High\n7  Doctor unavailable in community clinic. Multip...  Medical    High\n8  Request for blankets and warm clothes in hilly...  Shelter  Medium\n9  Family of 4 needs urgent evacuation from water...  Medical    High","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>type</th>\n      <th>urgency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Flood waters rising in Siliguri, 3 families tr...</td>\n      <td>Rescue</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Urgent requirement of medical aid for collapse...</td>\n      <td>Medical</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Children hungry, no food for 2 days in relief ...</td>\n      <td>Food</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Need shelter urgently after heavy rains in Meg...</td>\n      <td>Shelter</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Water pipes broken, entire slum has no clean d...</td>\n      <td>Medical</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Food packets needed in Malda. People haven’t e...</td>\n      <td>Food</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Flood rescue team needed in Aligarh. Locals st...</td>\n      <td>Food</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Doctor unavailable in community clinic. Multip...</td>\n      <td>Medical</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Request for blankets and warm clothes in hilly...</td>\n      <td>Shelter</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Family of 4 needs urgent evacuation from water...</td>\n      <td>Medical</td>\n      <td>High</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"**Geolocation**","metadata":{}},{"cell_type":"code","source":"!pip install -q geopy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:00.273235Z","iopub.execute_input":"2025-07-01T09:14:00.273446Z","iopub.status.idle":"2025-07-01T09:14:03.931712Z","shell.execute_reply.started":"2025-07-01T09:14:00.273430Z","shell.execute_reply":"2025-07-01T09:14:03.930746Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from geopy.geocoders import Nominatim\nimport time\n\ngeolocator = Nominatim(user_agent=\"crisiscompass\")\n\ndef get_coordinates(location_text):\n    try:\n        location = geolocator.geocode(location_text, timeout=10)\n        if location:\n            return location.latitude, location.longitude\n        else:\n            return None, None\n    except:\n        return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:03.932945Z","iopub.execute_input":"2025-07-01T09:14:03.933215Z","iopub.status.idle":"2025-07-01T09:14:04.340791Z","shell.execute_reply.started":"2025-07-01T09:14:03.933191Z","shell.execute_reply":"2025-07-01T09:14:04.340239Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Simple heuristic: extract last place-like word (manually or via LLM later)\ndf_preds[\"place\"] = df_preds[\"text\"].str.extract(r\"in ([A-Z][a-zA-Z]+)\")\n\n# Apply geolocation\ndf_preds[[\"latitude\", \"longitude\"]] = df_preds[\"place\"].apply(lambda x: pd.Series(get_coordinates(x)))\n\ndf_preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:04.341503Z","iopub.execute_input":"2025-07-01T09:14:04.341972Z","iopub.status.idle":"2025-07-01T09:14:13.488494Z","shell.execute_reply.started":"2025-07-01T09:14:04.341953Z","shell.execute_reply":"2025-07-01T09:14:13.487871Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                text     type urgency  \\\n0  Flood waters rising in Siliguri, 3 families tr...   Rescue    High   \n1  Urgent requirement of medical aid for collapse...  Medical    High   \n2  Children hungry, no food for 2 days in relief ...     Food  Medium   \n3  Need shelter urgently after heavy rains in Meg...  Shelter  Medium   \n4  Water pipes broken, entire slum has no clean d...  Medical  Medium   \n5  Food packets needed in Malda. People haven’t e...     Food  Medium   \n6  Flood rescue team needed in Aligarh. Locals st...     Food    High   \n7  Doctor unavailable in community clinic. Multip...  Medical    High   \n8  Request for blankets and warm clothes in hilly...  Shelter  Medium   \n9  Family of 4 needs urgent evacuation from water...  Medical    High   \n\n       place   latitude  longitude  \n0   Siliguri  26.716413  88.430992  \n1     Kanpur  26.460914  80.321759  \n2        NaN  34.220389  70.380031  \n3  Meghalaya  25.537943  91.299910  \n4        NaN  34.220389  70.380031  \n5      Malda  25.005745  88.139848  \n6    Aligarh  27.876107  78.135815  \n7        NaN  34.220389  70.380031  \n8        NaN  34.220389  70.380031  \n9    Cuttack  20.468600  85.879200  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>type</th>\n      <th>urgency</th>\n      <th>place</th>\n      <th>latitude</th>\n      <th>longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Flood waters rising in Siliguri, 3 families tr...</td>\n      <td>Rescue</td>\n      <td>High</td>\n      <td>Siliguri</td>\n      <td>26.716413</td>\n      <td>88.430992</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Urgent requirement of medical aid for collapse...</td>\n      <td>Medical</td>\n      <td>High</td>\n      <td>Kanpur</td>\n      <td>26.460914</td>\n      <td>80.321759</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Children hungry, no food for 2 days in relief ...</td>\n      <td>Food</td>\n      <td>Medium</td>\n      <td>NaN</td>\n      <td>34.220389</td>\n      <td>70.380031</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Need shelter urgently after heavy rains in Meg...</td>\n      <td>Shelter</td>\n      <td>Medium</td>\n      <td>Meghalaya</td>\n      <td>25.537943</td>\n      <td>91.299910</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Water pipes broken, entire slum has no clean d...</td>\n      <td>Medical</td>\n      <td>Medium</td>\n      <td>NaN</td>\n      <td>34.220389</td>\n      <td>70.380031</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Food packets needed in Malda. People haven’t e...</td>\n      <td>Food</td>\n      <td>Medium</td>\n      <td>Malda</td>\n      <td>25.005745</td>\n      <td>88.139848</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Flood rescue team needed in Aligarh. Locals st...</td>\n      <td>Food</td>\n      <td>High</td>\n      <td>Aligarh</td>\n      <td>27.876107</td>\n      <td>78.135815</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Doctor unavailable in community clinic. Multip...</td>\n      <td>Medical</td>\n      <td>High</td>\n      <td>NaN</td>\n      <td>34.220389</td>\n      <td>70.380031</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Request for blankets and warm clothes in hilly...</td>\n      <td>Shelter</td>\n      <td>Medium</td>\n      <td>NaN</td>\n      <td>34.220389</td>\n      <td>70.380031</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Family of 4 needs urgent evacuation from water...</td>\n      <td>Medical</td>\n      <td>High</td>\n      <td>Cuttack</td>\n      <td>20.468600</td>\n      <td>85.879200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"****RETRAINING ON NEW DATASET****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_synthetic = pd.read_csv(\"/kaggle/input/1000-entries-synthetic-dataset/synthetic_crisis_1000.csv\")\n\ndf_synthetic.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:13.489184Z","iopub.execute_input":"2025-07-01T09:14:13.489379Z","iopub.status.idle":"2025-07-01T09:14:13.508427Z","shell.execute_reply.started":"2025-07-01T09:14:13.489363Z","shell.execute_reply":"2025-07-01T09:14:13.507856Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                text     type urgency  \\\n0  Lack of basic medical services reported in Jha...  Medical  Medium   \n1  Local authorities in Goa report food issues, r...     Food  Medium   \n2     Urgent Medical assistance required in Gujarat.  Medical  Medium   \n3  Crisis alert: Shelter required in Madhya Prade...  Shelter    High   \n4  Food emergency reported in Odisha, situation i...     Food  Medium   \n\n         location  \n0       Jharkhand  \n1             Goa  \n2         Gujarat  \n3  Madhya Pradesh  \n4          Odisha  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>type</th>\n      <th>urgency</th>\n      <th>location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lack of basic medical services reported in Jha...</td>\n      <td>Medical</td>\n      <td>Medium</td>\n      <td>Jharkhand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Local authorities in Goa report food issues, r...</td>\n      <td>Food</td>\n      <td>Medium</td>\n      <td>Goa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Urgent Medical assistance required in Gujarat.</td>\n      <td>Medical</td>\n      <td>Medium</td>\n      <td>Gujarat</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Crisis alert: Shelter required in Madhya Prade...</td>\n      <td>Shelter</td>\n      <td>High</td>\n      <td>Madhya Pradesh</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Food emergency reported in Odisha, situation i...</td>\n      <td>Food</td>\n      <td>Medium</td>\n      <td>Odisha</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntype_encoder = LabelEncoder()\nurgency_encoder = LabelEncoder()\n\ndf_synthetic[\"type_encoded\"] = type_encoder.fit_transform(df_synthetic[\"type\"])\ndf_synthetic[\"urgency_encoded\"] = urgency_encoder.fit_transform(df_synthetic[\"urgency\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:13.509051Z","iopub.execute_input":"2025-07-01T09:14:13.509220Z","iopub.status.idle":"2025-07-01T09:14:13.515001Z","shell.execute_reply.started":"2025-07-01T09:14:13.509207Z","shell.execute_reply":"2025-07-01T09:14:13.514346Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:13.515665Z","iopub.execute_input":"2025-07-01T09:14:13.515932Z","iopub.status.idle":"2025-07-01T09:14:13.673266Z","shell.execute_reply.started":"2025-07-01T09:14:13.515916Z","shell.execute_reply":"2025-07-01T09:14:13.672508Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CrisisDataset(Dataset):\n    def __init__(self, texts, type_labels, urgency_labels, tokenizer):\n        self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n        self.labels_type = torch.tensor(type_labels)\n        self.labels_urgency = torch.tensor(urgency_labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item[\"labels_type\"] = self.labels_type[idx]\n        item[\"labels_urgency\"] = self.labels_urgency[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels_type)\n\ndataset = CrisisDataset(\n    texts=df_synthetic[\"text\"].tolist(),\n    type_labels=df_synthetic[\"type_encoded\"].tolist(),\n    urgency_labels=df_synthetic[\"urgency_encoded\"].tolist(),\n    tokenizer=tokenizer\n)\n\nloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:13.674369Z","iopub.execute_input":"2025-07-01T09:14:13.674926Z","iopub.status.idle":"2025-07-01T09:14:13.710915Z","shell.execute_reply.started":"2025-07-01T09:14:13.674900Z","shell.execute_reply":"2025-07-01T09:14:13.710326Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import DistilBertModel\nfrom torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\n\nclass CrisisClassifier(nn.Module):\n    def __init__(self, num_types, num_urgencies):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.dropout = nn.Dropout(0.3)\n        self.type_head = nn.Linear(self.bert.config.hidden_size, num_types)\n        self.urgency_head = nn.Linear(self.bert.config.hidden_size, num_urgencies)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = self.dropout(output.last_hidden_state[:, 0])\n        return self.type_head(pooled), self.urgency_head(pooled)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CrisisClassifier(\n    num_types=len(type_encoder.classes_),\n    num_urgencies=len(urgency_encoder.classes_)\n).to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = CrossEntropyLoss()\n\nfor epoch in range(30):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels_type = batch[\"labels_type\"].to(device)\n        labels_urgency = batch[\"labels_urgency\"].to(device)\n\n        out_type, out_urgency = model(input_ids, attention_mask)\n        loss_type = loss_fn(out_type, labels_type)\n        loss_urgency = loss_fn(out_urgency, labels_urgency)\n        loss = loss_type + loss_urgency\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1} — Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:14:13.711589Z","iopub.execute_input":"2025-07-01T09:14:13.711769Z","iopub.status.idle":"2025-07-01T09:15:37.663237Z","shell.execute_reply.started":"2025-07-01T09:14:13.711751Z","shell.execute_reply":"2025-07-01T09:15:37.662418Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 — Loss: 218.6211\nEpoch 2 — Loss: 108.3425\nEpoch 3 — Loss: 95.1508\nEpoch 4 — Loss: 90.3366\nEpoch 5 — Loss: 88.6875\nEpoch 6 — Loss: 86.9046\nEpoch 7 — Loss: 86.5311\nEpoch 8 — Loss: 85.8152\nEpoch 9 — Loss: 83.3219\nEpoch 10 — Loss: 81.0290\nEpoch 11 — Loss: 78.0140\nEpoch 12 — Loss: 75.5215\nEpoch 13 — Loss: 74.9160\nEpoch 14 — Loss: 71.4621\nEpoch 15 — Loss: 67.5391\nEpoch 16 — Loss: 65.1883\nEpoch 17 — Loss: 61.6045\nEpoch 18 — Loss: 59.4062\nEpoch 19 — Loss: 59.4905\nEpoch 20 — Loss: 56.6347\nEpoch 21 — Loss: 53.1783\nEpoch 22 — Loss: 48.7062\nEpoch 23 — Loss: 50.8290\nEpoch 24 — Loss: 48.7917\nEpoch 25 — Loss: 47.3531\nEpoch 26 — Loss: 43.3067\nEpoch 27 — Loss: 44.3194\nEpoch 28 — Loss: 43.8195\nEpoch 29 — Loss: 41.9629\nEpoch 30 — Loss: 41.3309\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# **FAIS similarity search**","metadata":{}},{"cell_type":"code","source":"!pip install -q sentence-transformers faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:15:37.664173Z","iopub.execute_input":"2025-07-01T09:15:37.664992Z","iopub.status.idle":"2025-07-01T09:17:02.738756Z","shell.execute_reply.started":"2025-07-01T09:15:37.664963Z","shell.execute_reply":"2025-07-01T09:17:02.737595Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:17:02.740026Z","iopub.execute_input":"2025-07-01T09:17:02.740364Z","iopub.status.idle":"2025-07-01T09:17:13.175922Z","shell.execute_reply.started":"2025-07-01T09:17:02.740328Z","shell.execute_reply":"2025-07-01T09:17:13.175105Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d63f0a00df43caaa6d3a4eab3a28a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffff743a2a49437f91ceca044444e93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef688839a04f4dbca47943f440b10f19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52624769e22c4f22aedd866f743d61d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f93e91cd2794a0f8f4cc55047a14cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae0a7e5d10742078e1f93cff841ef11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b3892d94604b1fa06b601826554061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"346b8d4531164eabb6ffc7ef3e5d40ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c184ade863724febbd3cd507207503f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5de602e31547a3a63fe9b2e85d11fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c993856d134b4f179fc9a9d2a1b75d7c"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"texts = df_synthetic[\"text\"].tolist()\n\nembeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:17:13.176779Z","iopub.execute_input":"2025-07-01T09:17:13.177072Z","iopub.status.idle":"2025-07-01T09:17:13.533716Z","shell.execute_reply.started":"2025-07-01T09:17:13.177049Z","shell.execute_reply":"2025-07-01T09:17:13.533175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c017f2beb1d74614b13adbfb709b7dd3"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import faiss\nimport numpy as np\n\ndim = embeddings.shape[1]\n\n# Create FAISS index\nindex = faiss.IndexFlatL2(dim)\nindex.add(embeddings)\n\n# Store original texts for easy retrieval\ncrisis_texts = df_synthetic[\"text\"].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:17:13.534564Z","iopub.execute_input":"2025-07-01T09:17:13.534842Z","iopub.status.idle":"2025-07-01T09:17:13.569174Z","shell.execute_reply.started":"2025-07-01T09:17:13.534818Z","shell.execute_reply":"2025-07-01T09:17:13.568634Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def search_similar_crises(query, top_k=5):\n    query_vec = embedder.encode([query], convert_to_numpy=True)\n    D, I = index.search(query_vec, top_k)\n\n    print(f\"\\n🔎 Top {top_k} similar crisis reports for:\\n\\\"{query}\\\"\\n\")\n    for i in I[0]:\n        print(\"•\", crisis_texts[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:17:13.569832Z","iopub.execute_input":"2025-07-01T09:17:13.570084Z","iopub.status.idle":"2025-07-01T09:17:13.574624Z","shell.execute_reply.started":"2025-07-01T09:17:13.570059Z","shell.execute_reply":"2025-07-01T09:17:13.573632Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"search_similar_crises(\"Need urgent rescue in flood-hit delhi\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:19:41.507023Z","iopub.execute_input":"2025-07-01T09:19:41.507327Z","iopub.status.idle":"2025-07-01T09:19:41.532834Z","shell.execute_reply.started":"2025-07-01T09:19:41.507299Z","shell.execute_reply":"2025-07-01T09:19:41.532105Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed48b69a1f1a43ceb6e93ad2b8f2b1bb"}},"metadata":{}},{"name":"stdout","text":"\n🔎 Top 5 similar crisis reports for:\n\"Need urgent rescue in flood-hit delhi\"\n\n• Urgent Rescue assistance required in Delhi.\n• Rescue emergency reported in Delhi, situation is High.\n• Delhi facing rescue shortage, urgency level: Medium.\n• Volunteers needed for rescue support in Delhi.\n• Urgent Rescue assistance required in Uttar Pradesh.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!pip install -q spacy geopy\n!python -m spacy download en_core_web_sm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:32:05.101079Z","iopub.execute_input":"2025-07-01T09:32:05.101694Z","iopub.status.idle":"2025-07-01T09:32:17.556574Z","shell.execute_reply.started":"2025-07-01T09:32:05.101673Z","shell.execute_reply":"2025-07-01T09:32:17.555841Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import spacy\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Load SpaCy NER model\nnlp = spacy.load(\"en_core_web_sm\")\ngeolocator = Nominatim(user_agent=\"crisis-location-ner\")\n\n# Extract first GPE from text\ndef extract_location(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        if ent.label_ == \"GPE\":\n            return ent.text\n    return None\n\n# Geocode using Nominatim\ndef geocode_place(place):\n    try:\n        loc = geolocator.geocode(place + \", India\", timeout=10)\n        if loc:\n            return loc.latitude, loc.longitude\n        else:\n            return None, None\n    except:\n        return None, None\n\n# Full pipeline\ndef enrich_crisis_report(text):\n    location = extract_location(text)\n    lat, lng = geocode_place(location) if location else (None, None)\n    time.sleep(1)  # Respect Nominatim's 1 sec limit\n\n    return {\n        \"text\": text,\n        \"location_extracted\": location,\n        \"latitude\": lat,\n        \"longitude\": lng\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:32:33.183341Z","iopub.execute_input":"2025-07-01T09:32:33.184417Z","iopub.status.idle":"2025-07-01T09:32:33.847772Z","shell.execute_reply.started":"2025-07-01T09:32:33.184387Z","shell.execute_reply":"2025-07-01T09:32:33.847229Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"sample_texts = [\n    \"Flood in Siliguri has displaced many families.\",\n    \"Urgent medical need in Aligarh.\",\n    \"Children hungry in a village near Bhuj after cyclone.\",\n    \"Rescue required in Dharavi, Mumbai.\",\n    \"Snake bite in rural Nagaland, help needed.\"\n]\n\nenriched = [enrich_crisis_report(text) for text in sample_texts]\n\nimport pandas as pd\ndf_enriched = pd.DataFrame(enriched)\ndf_enriched\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:32:41.656335Z","iopub.execute_input":"2025-07-01T09:32:41.657083Z","iopub.status.idle":"2025-07-01T09:32:47.344766Z","shell.execute_reply.started":"2025-07-01T09:32:41.657057Z","shell.execute_reply":"2025-07-01T09:32:47.344027Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                                text location_extracted  \\\n0     Flood in Siliguri has displaced many families.           Siliguri   \n1                    Urgent medical need in Aligarh.            Aligarh   \n2  Children hungry in a village near Bhuj after c...               Bhuj   \n3                Rescue required in Dharavi, Mumbai.            Dharavi   \n4         Snake bite in rural Nagaland, help needed.           Nagaland   \n\n    latitude  longitude  \n0  26.716413  88.430992  \n1  27.876107  78.135815  \n2  23.247245  69.668339  \n3  19.044463  72.858618  \n4  26.163056  94.588491  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>location_extracted</th>\n      <th>latitude</th>\n      <th>longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Flood in Siliguri has displaced many families.</td>\n      <td>Siliguri</td>\n      <td>26.716413</td>\n      <td>88.430992</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Urgent medical need in Aligarh.</td>\n      <td>Aligarh</td>\n      <td>27.876107</td>\n      <td>78.135815</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Children hungry in a village near Bhuj after c...</td>\n      <td>Bhuj</td>\n      <td>23.247245</td>\n      <td>69.668339</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rescue required in Dharavi, Mumbai.</td>\n      <td>Dharavi</td>\n      <td>19.044463</td>\n      <td>72.858618</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Snake bite in rural Nagaland, help needed.</td>\n      <td>Nagaland</td>\n      <td>26.163056</td>\n      <td>94.588491</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Save model weights (after training)\ntorch.save(model.state_dict(), \"crisis_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:48:56.969452Z","iopub.execute_input":"2025-07-01T09:48:56.970183Z","iopub.status.idle":"2025-07-01T09:48:57.646291Z","shell.execute_reply.started":"2025-07-01T09:48:56.970162Z","shell.execute_reply":"2025-07-01T09:48:57.645540Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"**INSTRUCTIONS FOR FINE TUNING OF LLM**","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\ndf = pd.read_json(\"/kaggle/input/emergency-llm-dataset/Click here to download emergency_instructions_dataset.json\")\nwith open(\"instruction_dataset.jsonl\", \"w\") as f:\n    for _, row in df.iterrows():\n        json.dump({\"input\": row[\"input\"], \"output\": row[\"output\"]}, f)\n        f.write(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:15:52.430425Z","iopub.execute_input":"2025-07-01T17:15:52.430691Z","iopub.status.idle":"2025-07-01T17:15:52.846182Z","shell.execute_reply.started":"2025-07-01T17:15:52.430668Z","shell.execute_reply":"2025-07-01T17:15:52.845352Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def format_prompt(example):\n    return {\n        \"text\": f\"<s>[INST] {example['input']} [/INST] {example['output']}</s>\"\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:16:05.883303Z","iopub.execute_input":"2025-07-01T17:16:05.883571Z","iopub.status.idle":"2025-07-01T17:16:05.887748Z","shell.execute_reply.started":"2025-07-01T17:16:05.883547Z","shell.execute_reply":"2025-07-01T17:16:05.887012Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers datasets peft accelerate bitsandbytes\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# Config\nBASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nEPOCHS = 3\nBATCH_SIZE = 4\nLEARNING_RATE = 2e-4\n\n# Load dataset\ndataset = load_dataset(\"json\", data_files=\"/kaggle/input/emergency-llm-dataset/Click here to download emergency_instructions_dataset.json\")[\"train\"]\n\n# Format prompt for instruction-style models\ndef format_prompt(example):\n    return {\n        \"text\": f\"<s>[INST] {example['input']} [/INST] {example['output']}</s>\"\n    }\n\ndataset = dataset.map(format_prompt)\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token = tokenizer.eos_token  # safe padding\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    load_in_4bit=True,\n    torch_dtype=torch.float16\n)\n\n# Prep for LoRA\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Tokenize with labels (fixes missing loss issue)\ndef tokenize(example):\n    encoding = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )\n    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n    return encoding\n\ntokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n\n# Training setup\ntraining_args = TrainingArguments(\n    output_dir=\"./crisis_instruction_llama\",\n    per_device_train_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    fp16=True,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset\n)\n\n# Train!\ntrainer.train()\n\n# Save final model\ntrainer.save_model(\"crisis-instruction-tinyllama-lora\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:32:11.278008Z","iopub.execute_input":"2025-07-01T17:32:11.278391Z","iopub.status.idle":"2025-07-01T18:00:52.796408Z","shell.execute_reply.started":"2025-07-01T17:32:11.278360Z","shell.execute_reply":"2025-07-01T18:00:52.795808Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f13ff5669bd405ea0e6f02ed4ef88ae"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 28:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>7.295300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.673900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.324900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.135200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.106500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.087100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.079100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.075100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.073500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.071600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.070900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.070200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.068900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.068300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.066900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.066900</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.066900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.066200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.065700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.064800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.065300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.065500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.064300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.064400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.064700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.064500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.064200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.063600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.064400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.064000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.063700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.063900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.063900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.063600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.062800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.063700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.063500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.063300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.063300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.063500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.063100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.063000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.062800</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.062900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.062800</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.062600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.062100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.061900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.062000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.062100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"crisis-instruction-tinyllama-lora\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nprompt = \"Crisis Type: Medical, Urgency: Low, Location: Goa, Contact: +91-832-1234567\"\n\nresult = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)[0][\"generated_text\"]\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:04:04.558571Z","iopub.execute_input":"2025-07-01T18:04:04.559075Z","iopub.status.idle":"2025-07-01T18:04:06.923793Z","shell.execute_reply.started":"2025-07-01T18:04:04.559051Z","shell.execute_reply":"2025-07-01T18:04:06.923046Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Crisis Type: Medical, Urgency: Low, Location: Goa, Contact: +91-832-1234567 [/contact] Medical situation in Goa is currently under control. Contact: +91-832-1234567.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Save the final fine-tuned model + tokenizer\nsave_path = \"CrisisCompass/llm_model\"\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(f\"✅ Model and tokenizer saved to: {save_path}\")\n`","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:05:33.668759Z","iopub.execute_input":"2025-07-01T18:05:33.669029Z","iopub.status.idle":"2025-07-01T18:05:33.937056Z","shell.execute_reply.started":"2025-07-01T18:05:33.669011Z","shell.execute_reply":"2025-07-01T18:05:33.936426Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"('CrisisCompass/llm_model/tokenizer_config.json',\n 'CrisisCompass/llm_model/special_tokens_map.json',\n 'CrisisCompass/llm_model/tokenizer.model',\n 'CrisisCompass/llm_model/added_tokens.json',\n 'CrisisCompass/llm_model/tokenizer.json')"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"crisis_llm_model\", 'zip', \"CrisisCompass/llm_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:07:43.747381Z","iopub.execute_input":"2025-07-01T18:07:43.747960Z","iopub.status.idle":"2025-07-01T18:07:44.325471Z","shell.execute_reply.started":"2025-07-01T18:07:43.747935Z","shell.execute_reply":"2025-07-01T18:07:44.324877Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/crisis_llm_model.zip'"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# Final Pipeline","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nimport spacy\nfrom geopy.geocoders import Nominatim\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DistilBertTokenizer, pipeline\nfrom transformers import DistilBertModel\nimport os\n\n# ========== 0. Load Custom Crisis Classifier ==========\nclass CrisisClassifier(nn.Module):\n    def __init__(self, num_types=5, num_urgencies=3):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.dropout = nn.Dropout(0.3)\n        self.type_head = nn.Linear(self.bert.config.hidden_size, num_types)\n        self.urgency_head = nn.Linear(self.bert.config.hidden_size, num_urgencies)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = self.dropout(output.last_hidden_state[:, 0])\n        return self.type_head(pooled), self.urgency_head(pooled)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclassifier_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nclassifier_model_path = \"/kaggle/input/triage-basic-gen/transformers/default/1/crisis_model.pt\"\nclassifier_model = CrisisClassifier()\nclassifier_model.load_state_dict(torch.load(classifier_model_path, map_location=device))\nclassifier_model.to(device)\nclassifier_model.eval()\n\ndef classify_crisis(text):\n    inputs = classifier_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    out_type, out_urgency = classifier_model(**inputs)\n    type_idx = torch.argmax(out_type, dim=1).item()\n    urgency_idx = torch.argmax(out_urgency, dim=1).item()\n    type_labels = [\"Medical\", \"Food\", \"Shelter\", \"Search & Rescue\", \"Infrastructure Damage\"]\n    urgency_labels = [\"Low\", \"Medium\", \"High\"]\n    return type_labels[type_idx], urgency_labels[urgency_idx]\n\n# ========== 1. Load Fine-tuned LLM ==========\nLLM_PATH = \"/kaggle/input/crisis-compass-instruction-llm\"\nllm_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)\nllm_model = AutoModelForCausalLM.from_pretrained(LLM_PATH)\nllm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, device=0 if torch.cuda.is_available() else -1)\n\n# ========== 2. Named Entity Recognition ==========\nnlp = spacy.load(\"en_core_web_sm\")\ndef extract_location(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        if ent.label_ in [\"GPE\", \"LOC\"]:\n            return ent.text\n    return \"Unknown\"\n\n# ========== 3. Geolocation ==========\ndef geocode_location(location):\n    try:\n        loc = geolocator.geocode(location, addressdetails=True)\n        if loc:\n            lat = loc.latitude\n            lon = loc.longitude\n            full_address = loc.address\n            address_dict = loc.raw.get(\"address\", {})\n            state = address_dict.get(\"state\") or address_dict.get(\"region\") or \"Unknown\"\n            return lat, lon, full_address, state\n    except:\n        pass\n    return None, None, \"\", \"Unknown\"\n\n# ========== 4. Emergency Contact Mapping ==========\nwith open(\"/kaggle/input/emergency-contacts-per-state/emergency_contacts.json\", \"r\") as f:\n    contact_dict = json.load(f)\n\ndef get_contact(state):\n    return contact_dict.get(state, \"Not Available\")\n\n# ========== 5. Instruction Generation ==========\ndef generate_instruction(crisis_type, urgency, location, contact):\n    prompt = f\"<s>[INST] Crisis Type: {crisis_type}, Urgency: {urgency}, Location: {location}, Contact: {contact} [/INST]\"\n    result = llm_pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n    return result[0][\"generated_text\"].split(\"[/INST]\")[-1].strip()\n\n# ========== 6. Full Pipeline ==========\ndef crisis_pipeline(report_text):\n    crisis_type, urgency = classify_crisis(report_text)\n    location = extract_location(report_text)\n    lat, lon, full_address, state = geocode_location(location)\n    contact = get_contact(state)\n    instruction = generate_instruction(crisis_type, urgency, location, contact)\n\n    return {\n        \"crisis_type\": crisis_type,\n        \"urgency\": urgency,\n        \"location\": location,\n        \"state\": state,\n        \"latitude\": lat,\n        \"longitude\": lon,\n        \"contact\": contact,\n        \"instruction\": instruction\n    }\n\n# ========== 7. Example ==========\nif __name__ == \"__main__\":\n    report = \"Massive flood near Bhagalpur. People are stranded on rooftops. Immediate help needed.\"\n    result = crisis_pipeline(report)\n    print(json.dumps(result, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T05:33:38.563643Z","iopub.execute_input":"2025-07-02T05:33:38.564384Z","iopub.status.idle":"2025-07-02T05:33:44.631648Z","shell.execute_reply.started":"2025-07-02T05:33:38.564358Z","shell.execute_reply":"2025-07-02T05:33:44.630743Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"{\n  \"crisis_type\": \"Search & Rescue\",\n  \"urgency\": \"Medium\",\n  \"location\": \"Bhagalpur\",\n  \"state\": \"Bihar\",\n  \"latitude\": 25.2861354,\n  \"longitude\": 87.1304229,\n  \"contact\": \"+91-612-11223344\",\n  \"instruction\": \"Search & Rescue issue in Bhagalpur. Contact: +91-612-1122334.\"\n}\n","output_type":"stream"}],"execution_count":9}]}